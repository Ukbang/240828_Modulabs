import streamlit as st
import os
from typing import Optional
from langchain_core.documents.base import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.vectorstores.base import VectorStoreRetriever
from langchain_community.vectorstores import FAISS
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda

# os.environ["openai_api_key"] = "Your API-Key"

st.set_page_config(page_title="ëª¨ë‘í•´ìœ ",
                   page_icon="ðŸ¤–")

st.title("ëª¨ë‘í•´ìœ  ê³ ê° ì‘ëŒ€ ì±—ë´‡ ë§Œë“¤ê¸°!")

st.markdown("""\n
ëª¨ë‘í•´ìœ  ì„¸ë¯¸ë‚˜ì— ì˜¤ì‹ ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!\n
ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.
""")

# íŒŒì¼ ìž„ë² ë”© ìˆ˜í–‰
@st.cache_resource(show_spinner="Loading...")
def embedding_file(file: str) -> VectorStoreRetriever:
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=300,
        chunk_overlap=100,
        separators=["\n\n"],
    )

    loader = PyPDFLoader(f"./files/{file}")
    docs = loader.load_and_split(text_splitter=splitter)

    embeddings = OpenAIEmbeddings()
    vector_store = FAISS.from_documents(docs, embeddings)
    retriever = vector_store.as_retriever()    

    return retriever

# íŒŒì¼ ì„ íƒ
with st.sidebar:
    chat_clear = None
    model_name = st.selectbox(label="ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", placeholder= "Select Your Model", options=["gpt-3.5-turbo-0125", "gpt-4o"], index=None)
    file = st.selectbox(label="íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", placeholder= "Select Your File", options=(os.listdir("./files")), index=None)
    if file:
        retriever = embedding_file(file)
        st.success(f"ìž„ë² ë”©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.info(f"í˜„ìž¬ ìž„ë² ë”© ëœ íŒŒì¼ëª… : \n\n **{file}**")
        col1, col2 = st.columns(2)
        chat_clear = col1.button("ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™”")
        embed_clear = col2.button("ìž„ë² ë”© ì´ˆê¸°í™”")

        llm = ChatOpenAI(temperature=0.1,
                        model=model_name)          

        if embed_clear:
            st.cache_resource.clear()

if "history" not in st.session_state or chat_clear:
    st.session_state["history"] = []    

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", 
     """
    context : {context}

    ë‹¹ì‹ ì€ ì–¸ì œë‚˜ ê³ ê°ì—ê²Œ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€ì„ í•˜ë©° ë§íˆ¬ëŠ” êµ‰ìž¥ížˆ ì¹œê·¼í•©ë‹ˆë‹¤. ì§ì—…ì€ ì „ë¬¸ ìƒë‹´ì›ìž…ë‹ˆë‹¤. ë‹µë³€ ì‹œ, ì•„ëž˜ì˜ ê·œì¹™ì„ ì§€ì¼œì•¼ë§Œ í•©ë‹ˆë‹¤.
    ê·œì¹™ 1. ì£¼ì–´ì§„ contextë§Œì„ ì´ìš©í•˜ì—¬ ë‹µë³€í•´ì•¼í•©ë‹ˆë‹¤. 
    ê·œì¹™ 2. ì£¼ì–´ì§„ contextì—ì„œ ë‹µë³€ì„ í•  ìˆ˜ ì—†ë‹¤ë©´ "í•´ë‹¹ ë¬¸ì˜ëŠ” 010-2255-3366ìœ¼ë¡œ ì—°ë½ì£¼ì‹œë©´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì˜ì—… ì‹œê°„ì€ ì˜¤ì „ 10ì‹œ-ì˜¤í›„ 6ì‹œìž…ë‹ˆë‹¤." ë¼ê³  ëŒ€ë‹µí•˜ì„¸ìš”.
    ê·œì¹™ 3. ë¬¸ìžì—´ì— A1, A2, A11, A22 ë“± í•„ìš” ì—†ëŠ” ë¬¸ìžëŠ” ì œê±°í•œ ë’¤ ì¶œë ¥í•©ë‹ˆë‹¤.
    ê·œì¹™ 4. í•­ìƒ ì¹œì ˆí•œ ë§íˆ¬ë¡œ ì‘ëŒ€í•©ë‹ˆë‹¤.
    ê·œì¹™ 5. ì›¹ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤. ëŒ€ì†Œë¬¸ìžë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì„¸ìš”.
    """),
    ("human", "{query}")
])

query = st.chat_input("ì§ˆë¬¸ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”.")

def format_docs(docs: list[Document]) -> str:
    return "\n\n".join(doc.page_content for doc in docs)


def chat_history(message: Optional[str] = None, role: Optional[str] = None, show: bool=True) -> None:
    if message and role:
        st.session_state["history"].append({"message":message, "role":role})   
    if show:
        for m in st.session_state["history"]:
            with st.chat_message(m["role"]):
                st.write(m["message"])

def chat_llm(query: str) -> None:
    chat_history(message=query, role="user", show=False)    
    chain = {"context":retriever | RunnableLambda(format_docs),
             "query":RunnablePassthrough()} | prompt | llm
    result = chain.invoke(query)
    chat_history(message=result.content, role = "ai")    

if query:
    chat_llm(query)
