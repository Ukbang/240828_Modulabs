import streamlit as st
import os
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda

os.environ["openai_api_key"] = "Your API-Key"

# "gpt-3.5-turbo-0125", "gpt-4o"

llm = ChatOpenAI(temperature=0.1,)

st.set_page_config(page_title="ëª¨ë‘í•´ìœ ",
                   page_icon="ğŸ¤–")

st.title("ëª¨ë‘í•´ìœ  ê³ ê° ì‘ëŒ€ ì±—ë´‡ ë§Œë“¤ê¸°!")

st.markdown("""\n
**ì™¼ìª½ ì‚¬ì´ë“œë°”ì— ì—¬ëŸ¬ë¶„ì˜ íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.**
            
**ì–´ëŠ ê²ƒì´ë˜ ë¬¼ì–´ë³´ì„¸ìš”!**
""")

# íŒŒì¼ ì—…ë¡œë”
with st.sidebar:
    file = st.file_uploader("PDFíŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", type="pdf")

# íŒŒì¼ ì„ë² ë”©
@st.cache_resource(show_spinner="Loading...")
def embed_file(file):

    splitter = CharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=300, # ë‚˜ëˆŒ ë¬¸ìëŠ” 300ê°œ
    chunk_overlap=100, # ê²¹ì¹  ë¬¸ìëŠ” 100ê°œ
    separator='\n',)

    loader = UnstructuredFileLoader(f"./files/{file}")

    docs = loader.load_and_split(text_splitter=splitter)

    embeddings = OpenAIEmbeddings()

    vector_store = FAISS.from_documents(docs, embeddings)

    retriever = vector_store.as_retriever()

    return retriever

# íŒŒì¼ ì½ì–´ì˜¤ê¸°
if file:
    file_content = file.read()
    file_path = f"./files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
    retriever = embed_file(file.name)

def combine_docs(docs):
    return "\n".join(doc.page_content for doc in docs)


st.session_state["history"] = []

def chat_history(message=None, role=None):
    try:
        with st.chat_message(role):     
            st.session_state["history"].append({"message":query, "role":role})
    except:
        with st.chat_message("ai"):
            st.error("ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
    for m in st.session_state["history"]:
        with st.chat_message(m["role"]):
            st.write(m["message"])




# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", 
     """
    ë‹¹ì‹ ì€ ì£¼ì–´ì§„ contextë§Œì„ ì´ìš©í•˜ì—¬ ë‹µë³€í•´ì•¼í•©ë‹ˆë‹¤. ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "í•´ë‹¹ ë¬¸ì˜ëŠ” ì œê°€ ë„ì™€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. 
    010-1234-1234ë¡œ ì—°ë½ì£¼ì‹œë©´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì˜ì—… ì‹œê°„ì€ ì˜¤ì „ 10ì‹œ ~ ì˜¤í›„ 6ì‹œì…ë‹ˆë‹¤." ë¼ê³  ëŒ€ë‹µí•˜ì„¸ìš”.

    context : {context}
    """),
    ("human", "{query}")
])


query = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

if query:
    chat_history(message=query, role="user")
    chain = {"context":retriever | RunnableLambda(combine_docs),
            "query":RunnablePassthrough()} | prompt | llm
    result = chain.invoke(query)
    chat_history(message=result.content, role = "ai")