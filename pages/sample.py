import streamlit as st
import time
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.embeddings import CacheBackedEmbeddings
from langchain_community.vectorstores import FAISS, chroma
from langchain.storage import LocalFileStore
from langchain.chains import RetrievalQA
from langchain.callbacks import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda

st.set_page_config(
    page_title="ModuhaeYou",
    page_icon="ğŸ…"
)

# @st.cache_data(show_spinner="ë‹µë³€ ìƒì„± ì¤‘...")
@st.cache_resource(show_spinner="Loading...")
def embed_file(file):

    splitter = CharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=600, # ë‚˜ëˆŒ ë¬¸ìëŠ” 600ê°œ
    chunk_overlap=100, # ê²¹ì¹  ë¬¸ìëŠ” 100ê°œ
    separator='\n',)

    loader = UnstructuredFileLoader(f"pages/.cache/{file}")

    docs = loader.load_and_split(text_splitter=splitter)

    cache_dir = LocalFileStore(f"pages/.cache/{file}")

    embeddings = OpenAIEmbeddings()

    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    embeddings, cache_dir)

    vector_store = FAISS.from_documents(docs, cached_embeddings)

    retriever = vector_store.as_retriever()

    return retriever

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        st.session_state["messages"].append({"message":message, "role":role})

def paint_history():
    if len(st.session_state["messages"]) <= 20:
        for message in st.session_state["messages"]:
            send_message(message["message"], message["role"], save=False)
    else:
        for message in st.session_state["messages"][-20:]:
            send_message(message["message"], message["role"], save=False)        

def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

class ChatCallbackHandler(BaseCallbackHandler):

    message = ""

    def on_llm_start(self, *arg, **kwargs):
        self.message_box = st.empty()
    
    def on_llm_end(self, *arg, **kwargs):
        st.session_state["messages"].append({"message":self.message, "role":"ai"})

    def on_llm_new_token(self, token, *args, **klwargs):
        self.message += token
        self.message_box.markdown(self.message)

chat = ChatOpenAI(
    temperature=0.1,
    # model="gpt-4-0125-preview",
    streaming=True,
    callbacks=[
        ChatCallbackHandler()
    ]
)

prompt = ChatPromptTemplate.from_messages([
    ("system", """
    ë‹¤ìŒ ë¬¸ë§¥ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œì— ë‹µí•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ì´ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ê³  ë§í•˜ì„¸ìš”. ë‹µë³€ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
    ê·¸ë¦¬ê³  ëŒ€ë‹µì€ í•­ìƒ í•œê¸€ë¡œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”. Always Answer Only Korean.

     context : {context}
    """),
    ("human", "{question}")
])

st.title("Document GPT")

st.markdown("""
##### í™˜ì˜í•©ë‹ˆë‹¤!
            
**ì™¼ìª½ ì‚¬ì´ë“œë°”ì— ì—¬ëŸ¬ë¶„ì˜ íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.**
            
**ì–´ëŠ ê²ƒì´ë˜ ë¬¼ì–´ë³´ì„¸ìš”!**
""")
with st.sidebar:
    file = st.file_uploader("txtíŒŒì¼ í˜¹ì€ pdfíŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["pdf", "txt"])

if file:
    file_content = file.read()
    file_path = f"pages/.cache/{file.name}"
    
    with open(file_path, "wb") as f:
        f.write(file_content)
    retriever = embed_file(file.name)

    send_message("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!", "ai", save=False)
    paint_history()
    message = st.chat_input("íŒŒì¼ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.")

    if message:
        send_message(message, "human")
        chain = {
            "context":retriever | RunnableLambda(format_docs), 
            "question":RunnablePassthrough()
        } | prompt | chat

        with st.chat_message("ai"):
            chain.invoke(message)

else:
    st.session_state["messages"] = []